{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rmh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_election_data(start_date, end_date, election_year, subdir):\n",
    "\n",
    "    data = []\n",
    "    dayrange = range((end_date - start_date).days + 1)\n",
    "\n",
    "    ARTICLES_DIR = join('data', 'guardian', subdir, election_year)\n",
    "\n",
    "    for daycount in dayrange:\n",
    "        dt = start_date + timedelta(days=daycount)\n",
    "        datestr = dt.strftime('%Y-%m-%d')\n",
    "        fname = join(ARTICLES_DIR, datestr + '.json')\n",
    "        with open(fname) as f:\n",
    "            for hd in json.load(f):\n",
    "                data.append({'headline': hd.lower(), 'date': datestr})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_aspects_with_fuzzy_match(aspects, news_hd):\n",
    "    asp_match = []\n",
    "    pos = []\n",
    "    lbls = []\n",
    "    hdls = []\n",
    "    dates = []\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for hd in news_hd:\n",
    "        \n",
    "        hd_text = hd['headline']\n",
    "        #re.sub(r'\\W+', '', hd_text)\n",
    "        hd_text = hd_text.replace('\\n','')\n",
    "        hd_text = hd_text.replace('\\t','')\n",
    "        hd_date = hd['date']\n",
    "        \n",
    "        for w in hd_text.split():\n",
    "            if w not in stopWords:\n",
    "                for kw in aspects:\n",
    "                    score = fuzz.partial_ratio(kw, w)\n",
    "\n",
    "                    if score > 80:\n",
    "                        s_idx = hd_text.find(w)\n",
    "                        asp_match.append(kw)\n",
    "                        pos.append(str(s_idx) + ',' + str(s_idx + len(w)))\n",
    "                        lbls.append('positive')\n",
    "                        hdls.append(hd_text)\n",
    "                        dates.append(hd_date)\n",
    "                        \n",
    "    return hdls, asp_match, lbls, pos, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(year, hds, pos, asp, lbl, dates, subdir):\n",
    "    \n",
    "    path = './data/processed-data/' + subdir + '/' + year\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "    {'headlines': hds,\n",
    "     'terms': asp,\n",
    "     'dates': dates\n",
    "    })\n",
    "    \n",
    "    df.to_csv(path + '/hds.csv', index=False)\n",
    "    \n",
    "    \n",
    "    with open(path + '/headlines.txt', \"w\") as output:\n",
    "        for row in hds:\n",
    "            output.write(str(row.rstrip()) + '\\n')\n",
    "\n",
    "    with open(path + '/position.txt', \"w\") as output:\n",
    "        for row in pos:\n",
    "            output.write(str(row) + '\\n')\n",
    "\n",
    "    with open(path + '/term.txt', \"w\") as output:\n",
    "        for row in asp:\n",
    "            output.write(str(row) + '\\n')\n",
    "\n",
    "    with open(path + '/label.txt', \"w\") as output:\n",
    "        for row in lbl:\n",
    "            output.write(str(row) + '\\n')\n",
    "            \n",
    "    with open(path + '/dates.txt', \"w\") as output:\n",
    "        for row in dates:\n",
    "            output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_trainable_data(start_date, end_date, year, fz_kw, subdir):\n",
    "    print('----' + year + '----------')\n",
    "    news_headlines = get_election_data(start_date, end_date, year, subdir)\n",
    "    headlines, aspects, labels, positions, dates = identify_aspects_with_fuzzy_match(fz_kw, news_headlines)\n",
    "    write_data(year, headlines, positions, aspects, labels, dates, subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n",
      "----2020----------\n"
     ]
    }
   ],
   "source": [
    "aspect_key_words_candidates_2016 = ['trump', 'donald', 'hillary', 'clinton']\n",
    "aspect_key_words_candidates_2012 = ['barack', 'obama', 'mitt', 'romney']\n",
    "aspect_key_words_candidates_2008 = ['barack', 'obama', 'john', 'mccain']\n",
    "aspect_key_words_candidates_2004 = ['george', 'bush', 'john', 'kerry']\n",
    "aspect_key_words_candidates_2000 = ['george', 'bush', 'gore']\n",
    "aspect_key_words_candidates_2020 = ['trump', 'donald', 'joe', 'biden']\n",
    "\n",
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', aspect_key_words_candidates_2016, 'candidates')\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', aspect_key_words_candidates_2012, 'candidates')\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', aspect_key_words_candidates_2008, 'candidates')\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', aspect_key_words_candidates_2004, 'candidates')\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', aspect_key_words_candidates_2000, 'candidates')\n",
    "gen_trainable_data(date(2020, 3, 12), date(2020, 6, 12), '2020', aspect_key_words_candidates_2020, 'candidates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n",
      "----2020----------\n"
     ]
    }
   ],
   "source": [
    "search_key_imm = ['immigration', 'undocumented', 'mexico', 'asylum', 'south', 'border', \n",
    "                  'deport', 'ICE', 'USCIS', 'refugee', 'migrant', 'visa', 'green', 'card']\n",
    "\n",
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', search_key_imm, 'immigration')\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', search_key_imm, 'immigration')\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', search_key_imm, 'immigration')\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', search_key_imm, 'immigration')\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', search_key_imm, 'immigration')\n",
    "gen_trainable_data(date(2020, 3, 12), date(2020, 6, 12), '2020', search_key_imm, 'immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n",
      "----2020----------\n"
     ]
    }
   ],
   "source": [
    "search_key_health = ['healthcare', 'insurance', 'coverage', 'prescription', 'preexisting', \n",
    "                     'condition', 'medicare', 'pocket', 'cost', 'aca', 'affordable', 'medicaid'] \n",
    "\n",
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', search_key_health, 'health')\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', search_key_health, 'health')\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', search_key_health, 'health')\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', search_key_health, 'health')\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', search_key_health, 'health')\n",
    "gen_trainable_data(date(2020, 3, 12), date(2020, 6, 12), '2020', search_key_health, 'health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n",
      "----2020----------\n"
     ]
    }
   ],
   "source": [
    "search_key_race = ['job', 'unemployment', 'race', 'black', 'white', 'discrimination'] \n",
    "\n",
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', search_key_race, 'jobs-race')\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', search_key_race, 'jobs-race')\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', search_key_race, 'jobs-race')\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', search_key_race, 'jobs-race')\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', search_key_race, 'jobs-race')\n",
    "gen_trainable_data(date(2020, 3, 12), date(2020, 6, 12), '2020', search_key_race, 'jobs-race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n",
      "----2020----------\n"
     ]
    }
   ],
   "source": [
    "search_key_env = ['global', 'warming', 'green', 'deal', 'environment', 'coal', 'job', 'fossil', 'fuel']\n",
    "\n",
    "\n",
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', search_key_env, 'environment')\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', search_key_env, 'environment')\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', search_key_env, 'environment')\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', search_key_env, 'environment')\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', search_key_env, 'environment')\n",
    "gen_trainable_data(date(2020, 3, 12), date(2020, 6, 12), '2020', search_key_env, 'environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n",
      "----2020----------\n"
     ]
    }
   ],
   "source": [
    "search_key_guns = ['gun', 'control', 'law', 'legislation', 'background', 'check', 'shooting', 'semi', 'automatic']\n",
    "\n",
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', search_key_guns, 'guns')\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', search_key_guns, 'guns')\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', search_key_guns, 'guns')\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', search_key_guns, 'guns')\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', search_key_guns, 'guns')\n",
    "gen_trainable_data(date(2020, 3, 12), date(2020, 6, 12), '2020', search_key_guns, 'guns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n",
      "----2020----------\n"
     ]
    }
   ],
   "source": [
    "search_key_party = ['democrat', 'republican', 'senate', 'house', 'campaign', 'super', 'pac', 'rallies', 'protest']\n",
    "\n",
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', search_key_party, 'party')\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', search_key_party, 'party')\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', search_key_party, 'party')\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', search_key_party, 'party')\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', search_key_party, 'party')\n",
    "gen_trainable_data(date(2020, 3, 12), date(2020, 6, 12), '2020', search_key_party, 'party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n",
      "----2020----------\n"
     ]
    }
   ],
   "source": [
    "search_key_economy = ['economy', 'gdp', 'stock', 'market', '401k', 'retirement', 'national', 'debt'\n",
    "                  'student', 'loan', 'bailout', 'bankruptcy', 'stimulus', 'stock', 'market', 'retirement',\n",
    "                     'trade', 'export', 'import', 'tax', 'manufacture', 'package', 'tariff']\n",
    "\n",
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', search_key_economy, 'economy')\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', search_key_economy, 'economy')\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', search_key_economy, 'economy')\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', search_key_economy, 'economy')\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', search_key_economy, 'economy')\n",
    "gen_trainable_data(date(2020, 3, 12), date(2020, 6, 12), '2020', search_key_economy, 'economy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('./data/processed-data/candidates/2016')\n",
    "#Path('./data/processed-data/candidates/2016').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
